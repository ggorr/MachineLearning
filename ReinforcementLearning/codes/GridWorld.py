from typing import Union, Sequence, Tuple

import numpy as np

action_symbol = 'â†â†’â†‘â†“'
action_letter = 'LRUD'


def random_index(prob: np.ndarray):
	"""
	í™•ë¥ ì— ë”°ë¼ indexë¥¼ ê²°ì •í•œë‹¤.
	pi=[0.1, 0.2, 0.3, 0.4]ì´ë©´ p(index=0)=0.1, p(index=1)=0.2 ë“±ë“±
	:param prob: í™•ë¥ 
	:return: index
	"""
	num = np.random.rand()  # num in [0,1)
	s = 0
	for i in range(prob.shape[0]):
		s += prob[i]
		if num < s:
			return i
	# prob = [0, 0, ..., 0]ì¸ ê²½ìš°
	raise RuntimeError


class Environment(object):
	@staticmethod
	def standard_prob(base_prob: float = 1.0):
		"""
		actionì— ëŒ€í•˜ì—¬ next stateë¥¼ ì„ íƒí•  í™•ë¥ 
		í•´ë‹¹ actionì— base_prob + (1-base_prob)/4, ë‚˜ë¨¸ì§€ actionì— (1-base_prob)/4ì˜ í™•ë¥ ì„ ë°°ì •
		:param base_prob: 1ì´ë©´ deterministic
		:return: í™•ë¥  í…Œì´ë¸”
		"""
		return np.zeros((4, 4)) + (1 - base_prob) / 4 + base_prob * np.eye(4, 4)

	def __init__(self, row: int = 4, col: int = 4, terminal: tuple = None, prob: np.ndarray = None, base_prob: float = 1.0):
		"""
		:param row: # of rows
		:param col: # of columns
		:param prob: í™•ë¥  í…Œì´ë¸”. í¸ì˜ìƒ ëª¨ë“  stateì—ì„œ í™•ë¥ ì´ ê°™ë‹¤ê³  ê°€ì •í•œë‹¤. ë”°ë¼ì„œ 4x4 maxtrix. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ row x col x 4 x 4 matrix
		:param terminal: tuple of tuples i.e. tuple of terminal states
		:param base_prob: í™•ë¥  í…Œì´ë¸” probì´ Noneì´ë©´ base_probìœ¼ë¡œë¶€í„° í™•ë¥ ì„ ê³„ì‚°
		"""
		self.row: int = row
		self.col: int = col
		# actionì„ ì‹¤í–‰í•˜ì˜€ì„ ë•Œ ì‹¤ì œ ì´ë™í•˜ëŠ” ë°©í–¥ì— ëŒ€í•œ í™•ë¥ 
		# ë™ìª½ìœ¼ë¡œ ì´ë™í•˜ì˜€ë‹¤ê³  ë°˜ë“œì‹œ ë™ìª½ìœ¼ë¡œ ì´ë™í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ì´ë™í•  ìˆ˜ë„ ìˆë‹¤
		self.prob: np.ndarray = Environment.standard_prob(base_prob) if prob is None else prob
		# stateì—ì„œ direction ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¼ ë•Œì˜ reward. ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ í•­ìƒ -1ì„ ì¤€ë‹¤
		self.reward: np.ndarray = np.zeros((row, col, 4)) - 1
		self.terminal = ((0, 0), (row - 1, col - 1)) if terminal is None else terminal
		for t in self.terminal:
			self.reward[t][:] = 0.0

	def __move_to_direction(self, state: tuple, direction: int):
		"""
		ê° ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ì˜€ì„ ë•Œì˜ state. actionì„ ì‹œí–‰í•˜ë©´ í™•ë¥ ì ìœ¼ë¡œ directionì´ ê²°ì •ëœë‹¤.
		:param state:
		:param direction:
		:return:
		"""
		# grid ë°–ìœ¼ë¡œ ë‚˜ê°ˆ ìˆ˜ëŠ” ì—†ë‹¤
		if direction == 0:  # left
			return state[0], state[1] if state[1] == 0 else state[1] - 1
		elif direction == 1:  # right
			return state[0], state[1] if state[1] + 1 == self.col else state[1] + 1
		elif direction == 2:  # up
			return state[0] if state[0] == 0 else state[0] - 1, state[1]
		else:  # down
			return state[0] if state[0] + 1 == self.row else state[0] + 1, state[1]

	def apply_action(self, state: tuple, action: int):
		"""
		stateì—ì„œ actionì„ ì ìš©í•˜ì—¬ ë‹¤ìŒ stateë¥¼ ì–»ëŠ”ë‹¤.
		:param state:
		:param action:
		:return: rewardì™€ ë‹¤ìŒ state. terminal stateì´ë©´ None, None
		"""
		if state in self.terminal:
			return None, None
		# ì‹¤ì œë¡œ ì´ë™í•  ë°©í–¥ì„ ê²°ì •í•œë‹¤
		direction = random_index(self.prob[action])
		return self.reward[state][direction], self.__move_to_direction(state, direction)

	def qvalue_from_stable(self, state: tuple, action: int, stable: np.ndarray, discount: float = 1.0) -> float:
		"""
		state value tableë¡œë¶€í„° action valueë¥¼ ê³„ì‚°í•œë‹¤
		ë³¸ë˜
			q(s, a) â† âˆ‘_(sâ€²,r) p(sâ€²,râ”‚s,ğ‘) [r + ğ›¾ v(sâ€²)]
		ì´ì§€ë§Œ ì´ í”„ë¡œê·¸ë¨ì—ì„œëŠ”
			q(s, a) â† âˆ‘_sâ€² p(sâ€²â”‚s,ğ‘) [r + ğ›¾ v(sâ€²)]
		ì´ë‹¤. ì´ ê°’ì„ returní•œë‹¤.

		:param state: state to be evaluated
		:param action: action to be evaluated
		:param stable: state value table = state values
		:param discount: discount rate
		:return: action value âˆ‘_sâ€² ğ‘Ÿ p(sâ€²â”‚s,ğ‘) [r + ğ›¾ v_ğœ‹(sâ€²)]
		"""

		if state in self.terminal:
			return 0.0
		v = 0.0
		for direction in range(4):
			v += self.prob[action, direction] * (self.reward[state][direction] + discount * stable[self.__move_to_direction(state, direction)])
		return v

	def policy_from_stable(self, stable: np.ndarray, epsilon: float = 0.0, discount: float = 1.0, delta: float = 0.0001) -> np.ndarray:
		"""
		derives ğœ–-greedy policy from state values as policy improvement
		policy is greedy if ğœ–=0[DEFALUT]
		:param stable: state values
		:param epsilon: ğœ– for ğœ–-greedy
		:param discount: discount rate
		:param delta: if |x-y| <= delta then x=y
		:return: the ğœ–-greedy policy derived from state values
		"""
		pi = np.zeros((self.row, self.col, 4)) + epsilon / 4
		v = np.empty(4)
		for i in range(self.row):
			for j in range(self.col):
				if (i, j) in self.terminal:
					pi[i, j, :] = 0.0
				else:
					for k in range(4):
						v[k] = self.qvalue_from_stable((i, j), k, stable, discount)
					tf = v >= np.max(v) - delta
					# maximaì— ê· ë“± ë¶„ë°°
					pi[i, j][tf] += (1 - epsilon) / np.count_nonzero(tf)
		return pi

	def policy_from_qtable(self, qtable: np.ndarray, epsilon: float = 0.0, delta: float = 0.0001) -> np.ndarray:
		"""
		:param qtable: q values
		:param epsilon: ğœ– for ğœ–-greedy
		:param delta: if |x-y| <= delta then x=y
		:return: the ğœ–-greedy policy derived from q
		"""
		pi = np.zeros((self.row, self.col, 4)) + epsilon / 4
		for i in range(self.row):
			for j in range(self.col):
				if (i, j) in self.terminal:
					pi[i, j, :] = 0.0
				else:
					tf = qtable[i, j] >= np.max(qtable[i, j]) - delta
					# maximaì— ê· ë“± ë¶„ë°°
					pi[i, j][tf] += (1 - epsilon) / np.count_nonzero(tf)
		return pi

	def uniform_policy(self):
		pi = np.zeros((self.row, self.col, 4)) + 0.25
		for t in self.terminal:
			pi[t][:] = 0.0
		return pi


class Agent(object):
	def __init__(self, row: int = 4, col: int = 4, pi: np.ndarray = None):
		"""
		:param row: # of rows
		:param col: # of columns
		:param pi: policy, ê³§ ê° stateì—ì„œ ê° ë°©í–¥ìœ¼ë¡œ ì´ë™í•  í™•ë¥ 
		"""
		self.row: int = row
		self.col: int = col
		self.pi: np.ndarray = pi

	def get_action(self, state: tuple):
		"""
		policyì— ë”°ë¼ actionì„ ì„ íƒí•œë‹¤.
		:param state:
		:return: action, 0, 1, 2, 3
		"""
		return random_index(self.pi[state])

	def svalue_from_stable(self, state: tuple, stable: np.ndarray, env: Environment, discount: float = 1.0):
		"""
		state valuesë¡œë¶€í„° state valueë¥¼ ê³„ì‚°í•œë‹¤.
		ë³¸ë˜
			v(s) âŸµ âˆ‘_ğ‘ ğœ‹(ğ‘â”‚s) âˆ‘_(sâ€²,r) p(sâ€²,ğ‘Ÿâ”‚ğ‘ ,ğ‘)[r + ğ›¾ v(sâ€²)]
		ì´ì§€ë§Œ ì´ í”„ë¡œê·¸ë¨ì—ì„œëŠ”
			v(s) âŸµ âˆ‘_ğ‘ ğœ‹(ğ‘â”‚s) âˆ‘_sâ€² p(sâ€²â”‚ğ‘ ,ğ‘)[r + ğ›¾ v(sâ€²)]
		:param state: state to be evaluated
		:param stable: state values
		:param env: environment
		:param discount: discount rate
		:return: state value âˆ‘_ğ‘ ğœ‹(ğ‘â”‚s) âˆ‘_sâ€² p(sâ€²â”‚ğ‘ ,ğ‘)[r + ğ›¾ v(sâ€²)]
		"""
		v = 0.0
		for k in range(4):
			# v(s) âŸµ âˆ‘_ğ‘ ğœ‹(ğ‘â”‚s) âˆ‘_sâ€² p(sâ€²â”‚ğ‘ ,ğ‘)[r + ğ›¾ v(sâ€²)]
			v += self.pi[state][k] * env.qvalue_from_stable(state, k, stable, discount)
		return v

	def stable_from_stable(self, stable: np.ndarray, env: Environment, discount: float = 1.0):
		"""
		evaluates state value table from state value table following given policy self.pi
		ë³¸ë˜
			v(s) âŸµ âˆ‘_ğ‘ ğœ‹(ğ‘â”‚s) âˆ‘_(sâ€²,r) p(sâ€²,ğ‘Ÿâ”‚ğ‘ ,ğ‘)[r + ğ›¾ v(sâ€²)]
		ì´ì§€ë§Œ ì´ í”„ë¡œê·¸ë¨ì—ì„œëŠ”
			v(s) âŸµ âˆ‘_ğ‘ ğœ‹(ğ‘â”‚s) âˆ‘_sâ€² p(sâ€²â”‚ğ‘ ,ğ‘)[r + ğ›¾ v(sâ€²)]
		:param stable: state values = state value table
		:param env: environment
		:param discount: discount rate
		:return: evaluated state values = state value table
		"""
		new_stable = np.empty_like(stable)
		for i in range(self.row):
			for j in range(self.col):
				new_stable[i, j] = self.svalue_from_stable((i, j), stable, env, discount)
		return new_stable

	def optimal_stable(self, env: Environment, stable: np.ndarray = None, discount: float = 1.0, err: float = 0.0001, display: tuple = ()):
		"""
		optimal state values for given policy self.pi
		:param stable: state values = state value table
		:param env: environment
		:param discount: discount rate
		:param err: evaluate until |v - v_previous| <= err
		:param display: epochs for print values
		:return: optimal state values and # of iteration
		"""
		# ì›ë³¸ ë³´ì¡´
		stable = np.zeros((self.row, self.col)) if stable is None else stable.copy()
		epoch = 0
		diff = np.inf
		while diff > err:
			if epoch in display:
				print(f'iteration {epoch}\n{stable}')
			stable_copy = stable.copy()
			for i in range(self.row):
				for j in range(self.col):
					stable[i, j] = self.svalue_from_stable((i, j), stable_copy, env, discount)
			diff = np.max(np.abs(stable - stable_copy))
			epoch += 1
		if epoch in display:
			print(f'iteration {epoch}\n{stable}')
		return stable, epoch

	def policy_string(self, delta: float = 0.0001):
		return Agent.policy_to_string(self.pi, delta)

	@staticmethod
	def policy_to_string(pi: np.ndarray, delta: float = 0.0001):
		"""
		:param pi: policy
		:param delta: if |x-y| <= delta then x=y
		:return:
		"""
		symbol = np.array(list(action_symbol))
		word = []
		for i in range(pi.shape[0]):
			for j in range(pi.shape[1]):
				m = np.max(pi[i, j])
				if m > 0:
					tf = pi[i, j] >= m - delta
					word.append(f'{"".join(symbol[tf]):5}')
				else:
					word.append(f'     ')
			word.append('\n')
		return ''.join(word)


class EpisodeGenerator(object):
	def __init__(self, env: Environment, agent: Agent):
		self.env: Environment = env
		self.agent: Agent = agent

	def start_state(self):
		"""
		:return: ì‹œì‘ state
		"""
		state = self.env.terminal[0]
		while state in self.env.terminal:
			state = np.random.randint(self.env.row), np.random.randint(self.env.col)
		return state

	def policy_next(self, state: tuple):
		"""
		:param state: current state.
		:return: actionê³¼ ê·¸ actionì„ ì ìš©í•œ reward, state. stateê°€ terminalì´ë©´ None, None, None
		"""
		if state in self.env.terminal:
			return None
		action = self.agent.get_action(state)
		reward, state = self.env.apply_action(state, action)
		return action, reward, state

	def policy_episode(self, state: Union[tuple, None] = None):
		"""
		:param state: ì‹œì‘ state. None(DEFAULT)ì´ë©´ ì‹œì‘ stateë¥¼ ë¬´ì‘ìœ„ë¡œ ì„¤ì •í•œë‹¤.
		:return: a full episode
		"""
		state = self.start_state() if state is None else state
		episode = [state]
		ars = self.policy_next(state)  # action, reward, state
		while ars is not None:
			episode += ars
			ars = self.policy_next(ars[2])  # action, reward, state
		return episode

	def q_next(self, qtable: np.ndarray, state: tuple, epsilon: float = 0.0, delta: float = 0.0001):
		"""
		chooses ğœ–-greedy action
		:param qtable: q-value table
		:param state: state
		:param epsilon: ğœ– for ğœ–-greedy
		:param delta: if |x-y| <= delta then x=y
		:return:  actionê³¼ ê·¸ actionì„ ì ìš©í•œ reward, state. stateê°€ terminalì´ë©´ None, None, None
		"""
		if state in self.env.terminal:
			return None
		prob = np.zeros(4) + epsilon / 4
		tf = qtable[state] >= np.max(qtable[state]) - delta
		prob[tf] += (1 - epsilon) / np.count_nonzero(tf)
		action = random_index(prob)
		reward, state = self.env.apply_action(state, action)
		return action, reward, state

	def q_episode(self, qtable: np.ndarray, state: tuple = None, epsilon: float = 0.0, delta: float = 0.0001):
		"""
		:param qtable: q-value table
		:param state: ì‹œì‘ state. None(DEFAULT)ì´ë©´ ì‹œì‘ stateë¥¼ ë¬´ì‘ìœ„ë¡œ ì„¤ì •í•œë‹¤.
		:param epsilon: ğœ– for ğœ–-greedy
		:param delta: if |x-y| <= delta then x=y
		:return: a full episode
		"""
		state = self.start_state() if state is None else state
		episode = [state]
		ars = self.q_next(qtable, state, epsilon, delta)  # action, reward, state
		while ars is not None:
			episode += ars
			ars = self.q_next(qtable, ars[2], epsilon, delta)  # action, reward, state
		return episode

	@staticmethod
	def to_string(episode: list):
		"""
		converts episode to string
		:param episode:
		:return:
		"""
		return ''.join([action_symbol[x] if isinstance(x, int) else str(x) for x in episode])


def policy_evaluation():
	row, col = 4, 4
	# discount rate ğ›¾
	discount = 1.0
	# deterministic environment, agentê°€ ì„ íƒí•œ ë°©í–¥ìœ¼ë¡œ ì´ë™í•œë‹¤
	env = Environment(row, col)
	#
	# env = Environment(row, col, terminal=((0, 0),))
	agent = Agent(row, col, env.uniform_policy())
	# ì¶œë ¥í•˜ëŠ” iteration
	display = (0, 1, 2, 3, 10, 100)
	# ì—°ì†ëœ ë‘ valueì˜ ì°¨ê°€ err ì´í•˜ê°€ ë ë•Œê¹Œì§€ evaluateí•œë‹¤
	stable, epoch = agent.optimal_stable(env, discount=discount, display=display, err=0.00000001)
	print(f'iteration {epoch}\n{stable}')


def policy_iteration():
	row, col = 4, 4
	# discount rate ğ›¾
	discount = 1.0
	# deterministic environment, agentê°€ ì„ íƒí•œ ë°©í–¥ìœ¼ë¡œ ì´ë™í•œë‹¤
	env = Environment(row, col)
	agent = Agent(row, col, env.uniform_policy())
	# ì¶œë ¥í•˜ëŠ” iteration
	display = (0, 1, 2, 3, 10, 100)
	epoch = 0
	pi_diff = np.inf
	while pi_diff > 0.0001:
		stable, _ = agent.optimal_stable(env, discount=discount)
		pi_copy = agent.pi
		# deltaëŠ” ì˜¤ì°¨ ë²”ìœ„. deltaê°€ í´ìˆ˜ë¡ ë§ì€ ë°©í–¥ì´ ê²€ì¶œë¨
		agent.pi = env.policy_from_stable(stable, discount=discount, delta=0.0001)
		pi_diff = np.max(np.abs(agent.pi - pi_copy))
		if epoch in display:
			print(f'iteration {epoch}\n{stable}')
		epoch += 1


def value_iteration():
	"""
	state value tableê³¼ policyë¥¼ ë™ì‹œì— ìµœì í™”í•œë‹¤.
	:return:
	"""
	row, col = 4, 4
	# deterministic environment, agentê°€ ì„ íƒí•œ ë°©í–¥ìœ¼ë¡œ ì´ë™í•œë‹¤
	env = Environment(row, col)
	# env = Environment(row, col, terminal=((0, 0),))
	# agentëŠ” ë„¤ ë°©í–¥ ì¤‘ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ ì´ë™í•œë‹¤
	# ì´ ì •ì±…ìœ¼ë¡œ ì‹œì‘í•˜ì—¬ optimalì— ë„ë‹¬
	agent = Agent(row, col, env.uniform_policy())
	# ì´ˆê¸° state valueëŠ” ëª¨ë‘ 0ìœ¼ë¡œ
	stable = np.zeros((row, col))
	# discount rate ğ›¾
	discount = 1.0
	# ì¶œë ¥í•˜ëŠ” iteration
	display = (0, 1, 2, 3, 10, 100)
	# state valueì˜ ë³€í™”ê°€ ë¯¸ë¯¸í•˜ë©´ ì¢…ë£Œ
	# diffëŠ” ë³€í™”ë¼
	diff = np.inf
	epoch = 0
	while diff > 0.0001:
		if epoch in display:
			print(f'iteration {epoch}\n{stable}')
			print(agent.policy_string())
		stable_copy = stable.copy()
		# state valuesì™€ policyë¡œë¶€í„° ìƒˆë¡œìš´ state valuesë¥¼ ê³„ì‚°í•œë‹¤
		# ğ‘£(ğ‘ ) âŸµ âˆ‘_ğ‘ ğœ‹(ğ‘â”‚ğ‘ ) âˆ‘_ğ‘ â€² ğ‘(ğ‘ â”‚ğ‘ ,ğ‘) [ğ‘Ÿ + ğ›¾ğ‘£(ğ‘ â€²)]
		stable = agent.stable_from_stable(stable_copy, env, discount=discount)
		diff = np.max(np.abs(stable - stable_copy))
		# policy evaluation
		# deltaëŠ” ì˜¤ì°¨ ë²”ìœ„. deltaê°€ í´ìˆ˜ë¡ ë§ì€ ë°©í–¥ì´ ê²€ì¶œë¨
		agent.pi = env.policy_from_stable(stable, discount=discount, delta=0.0001)
		epoch += 1
	print(f'iteration {epoch}\n{stable}')
	print(agent.policy_string())


def td0():
	"""
	given policyì— ëŒ€í•˜ì—¬ state value tableì„ ê³„ì‚°í•œë‹¤
	v(s) âŸµ v(s) + ğ›¼ [R + ğ›¾ v(s') - v(s)]
	:return:
	"""
	row, col = 4, 4
	# deterministic environment, agentê°€ ì„ íƒí•œ ë°©í–¥ìœ¼ë¡œ ì´ë™í•œë‹¤
	env = Environment(row, col, terminal=((0, 0), (row - 1, col - 1)))
	#
	# env = Environment(row, col, terminal=((0, 0),))
	# agentëŠ” ë„¤ ë°©í–¥ ì¤‘ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ ì´ë™í•œë‹¤
	# ì´ policyì— ëŒ€í•œ stateì˜ ê°€ì¹˜ë¥¼ ê³„ì‚°í•œë‹¤.
	agent = Agent(row, col, env.uniform_policy())
	# step size ğ›¼
	step_size = 0.01
	# discount rate ğ›¾
	discount = 1.0
	# state valueëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ì´ˆê¸°í™”
	stable = np.zeros((row, col))
	episode = EpisodeGenerator(env, agent)
	# ì¶œë ¥í•˜ëŠ” iteration
	display = (0, 1, 2, 3, 10, 100)
	epoch = 0
	while epoch < 30000:
		if epoch in display:
			print(f'iteration {epoch}\n{stable}')
		state = episode.start_state()
		ars = episode.policy_next(state)  # action, reward, next_state
		while ars is not None:
			# v(s) âŸµ v(s) + ğ›¼ [R + ğ›¾ v(s') - v(s)]
			stable[state] += step_size * (ars[1] + discount * stable[ars[2]] - stable[state])
			state = ars[2]
			ars = episode.policy_next(state)  # action, reward, next_state
		epoch += 1
	print(f'iteration {epoch}\n{stable}')


def q_learning():
	row, col = 4, 4
	# stochastic environment
	# Environment.standard_prob(0.6)ì€ ë‹¤ìŒê³¼ ê°™ë‹¤
	# [[.7, .1, .1, .1],
	#  [.1, .7, .1, .1],
	#  [.1, .1, .7, .1],
	#  [.1, .1, .1, .7]]
	# ë”°ë¼ì„œ leftë¡œ actionì„ ì·¨í•˜ë©´ ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ì´ë™í•  í™•ë¥ ì´ ê°ê° 0.1ì´ë‹¤.
	# right, up, downë„ ë§ˆì°¬ê°€ì§€
	env = Environment(row, col, terminal=((0, 0), (row - 1, col - 1)), prob=Environment.standard_prob(0.6))
	# policy-independent
	agent = Agent(row, col)
	# qëŠ” ëœë¤í•˜ê²Œ ì´ˆê¸°í™”
	# q = np.zeros((row, col, 4))
	# ì´ì–´ë„ ë¹„ìŠ·í•˜ë‹¤
	# qtable = np.random.rand(row, col, 4) - 1
	qtable = np.zeros((row, col, 4))
	# terminal stateì—ì„œëŠ” q-value(action value)ê°€ 0.0ì´ë‹¤
	for t in env.terminal:
		qtable[t][:] = 0.0
	# actionì€ ğœ–-greedyë¡œ ì„ íƒ
	# ğœ– for ğœ–-greedy
	epsilon = 0.1
	# step size ğ›¼
	step_size = 0.01
	# discount rate ğ›¾
	discount = 1.0
	gen = EpisodeGenerator(env, agent)
	# ì¶œë ¥í•˜ëŠ” iteration
	display = (0, 1, 2, 3, 10, 100)
	epoch = 0
	while epoch < 10000:
		if epoch in display:
			print(f'iteration {epoch}\n{np.array2string(qtable, precision=2)}')
			pi = env.policy_from_qtable(qtable, epsilon=0.0, delta=0.01)
			print(Agent.policy_to_string(pi))
		state = gen.start_state()
		ars = gen.q_next(qtable, state, epsilon)  # action, reward, next_state
		while ars is not None:
			# ğ‘(ğ‘ , ğ‘) âŸµ ğ‘(ğ‘ , ğ‘) + ğ›¼[ğ‘Ÿ + ğ›¾ max_ğ‘â€² â¡ğ‘(ğ‘ â€², ğ‘â€²) âˆ’ ğ‘(ğ‘ , ğ‘)]
			qtable[state][ars[0]] += step_size * (ars[1] + discount * np.max(qtable[ars[2]]) - qtable[state][ars[0]])
			state = ars[2]
			ars = gen.q_next(qtable, state, epsilon)  # action, reward, next_state
		epoch += 1
	print(f'iteration {epoch}\n{np.array2string(qtable, precision=2)}')
	# policyë¥¼ ê³„ì‚°í•œë‹¤
	# deltaëŠ” ì˜¤ì°¨ ë²”ìœ„. deltaê°€ í¬ë©´ ì—¬ëŸ¬ ë°©í–¥ì´ ê²€ì¶œëœë‹¤
	pi = env.policy_from_qtable(qtable, epsilon=0.0, delta=0.01)
	print(Agent.policy_to_string(pi))


def episode0():
	# policyì— ë”°ë¼ episode ìƒì„±
	env = Environment()
	agent = Agent(pi=env.uniform_policy())
	gen = EpisodeGenerator(env, agent)
	epi = gen.policy_episode()
	print(epi)
	print(EpisodeGenerator.to_string(epi))


def episode1():
	# qì— ë”°ë¼ episode ìƒì„±
	env = Environment()
	agent = Agent()
	q = np.random.rand(4, 4, 4) - 1
	gen = EpisodeGenerator(env, agent)
	epi = gen.q_episode(q, epsilon=0.1)
	print(epi)
	print(EpisodeGenerator.to_string(epi))


# policy_evaluation()
# policy_iteration()
# value_iteration()
# td0()
q_learning()
# episode0()
# episode1()
